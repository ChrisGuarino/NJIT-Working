{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajfsk_7JY3E"
      },
      "source": [
        "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be first graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). However, any given item may be worth 4 or 8 points; if an item is worth 8 points, you need to accordingly scale the 0-4 grade.\n",
        "\n",
        "\n",
        "The total score must be re-scaled to 100. That should apply to all future assignments so that Canvas assigns the same weight on all assignments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArgW_Vq-uTh"
      },
      "source": [
        "# Assignment 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKHAiVXNz-2i"
      },
      "source": [
        "### Preparation Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xtT_YAhmmXs"
      },
      "source": [
        "\n",
        "\n",
        "We will work with this [mystery dataset](https://drive.google.com/open?id=1WLnWBThCYZ25pReI5DCwk2bgDaCrJxI_&authuser=ikoutis%40njit.edu&usp=drive_fs) that you can download and place to your google drive. You can then put it somewhere on your google drive and bring it into your Colab by following the steps in the following cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AMaVk7jmn_D",
        "outputId": "97c546d7-e6a1-4418-fafa-b910b5b1bdf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsLbaghZrO-L"
      },
      "source": [
        "The file contains\n",
        "\n",
        "* Two matrices $X$ and $X_1$ of numerical features. These datasets have the same dimensions (169343x80) but they are different.\n",
        "* An array $y$ of labels, ranging from 0-39.\n",
        "* The indices $otrain$ of a training set. These indices tell you what rows of the arrays $X,X_1,y$ correspond to the training points. You can use these to make two different training sets $(X[train], y[train])$ and $(X_1[train], y[train])$\n",
        "* Similarly, it contains the indexes for a validation and a test set, $ovalid$ and $otest$ respectively.\n",
        "\n",
        "The following cell shows how to access these arrays and assign them to local numpy objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "HQ0Oyva2F3fF"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "mat = scipy.io.loadmat('mysteryDataset.mat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALSnNeSw0JoQ"
      },
      "source": [
        "## <font color = 'blue'> Question 1. Import the dataset and conver to torch tensors </font>\n",
        "\n",
        "Your task for this question is to adapt the above preparation steps, import all mentioned variables into numpy arrays, and then transform them to PyTorch tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "C20aEPMG0z2W"
      },
      "outputs": [],
      "source": [
        "type(mat.get('X'))\n",
        "X_feature = mat.get('X')\n",
        "X1_feature = mat.get('X1')\n",
        "y_labels = mat.get('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cast to Pytorch tensors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "X_feature = torch.tensor(X_feature, dtype=torch.float32)\n",
        "X1_feature = torch.tensor(X1_feature, dtype=torch.float32)\n",
        "y_labels = torch.tensor(y_labels, dtype=torch.long).squeeze() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor Shapes:\n",
            "X: torch.Size([169343, 80])\n",
            "X1: torch.Size([169343, 80])\n",
            "y: torch.Size([169343])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tensor Shapes:\\nX: {X_feature.shape}\\nX1: {X1_feature.shape}\\ny: {y_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 4,  5, 28,  ..., 10,  4,  1])"
            ]
          },
          "execution_count": 372,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "sBjMZF1wGaUp"
      },
      "outputs": [],
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 4)\n",
        "\n",
        "# G[1] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7I1lmGM0skC"
      },
      "source": [
        "## <font color = 'blue'> Question 2. Write a functioning classifier in PyTorch </font>\n",
        "\n",
        "Write code that defines a classification model for the above dataset, and all other functions that are needed for its training. Apply your model on the two datsets $X,X_1$ and report the accuracy. The classifier should operate on the GPU.\n",
        "\n",
        "**Hint:** Re-use code we discussed for the Softmax Regression module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class SoftMaxRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(80, 40)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        y = self.flatten(x)\n",
        "        y = self.linear(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X Split \n",
        " \n",
        "trainX_data, temp_data, trainX_labels, temp_labels = train_test_split(\n",
        "    X_feature, y_labels, test_size=0.4, random_state=42)  \n",
        "\n",
        "valX_data, testX_data, valX_labels, testX_labels = train_test_split(\n",
        "    temp_data, temp_labels, test_size=0.5, random_state=42)  \n",
        "\n",
        "# X1 Split\n",
        "\n",
        "trainX1_data, temp_data, trainX1_labels, temp_labels = train_test_split(\n",
        "    X1_feature, y_labels, test_size=0.4, random_state=42)  \n",
        "\n",
        "valX1_data, testX1_data, valX1_labels, testX1_labels = train_test_split(\n",
        "    temp_data, temp_labels, test_size=0.5, random_state=42) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Tensor Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#Dataset X\n",
        "trainX_dataset = TensorDataset(trainX_data, trainX_labels)\n",
        "valX_dataset = TensorDataset(valX_data, valX_labels)\n",
        "testX_dataset = TensorDataset(testX_data, testX_labels)\n",
        "\n",
        "#Dataset X1\n",
        "trainX1_dataset = TensorDataset(trainX1_data, trainX1_labels)\n",
        "valX1_dataset = TensorDataset(valX1_data, valX1_labels)\n",
        "testX1_dataset = TensorDataset(testX1_data, testX1_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "trainX_loader = DataLoader(trainX_dataset, batch_size=batch_size, shuffle=True)\n",
        "valX_loader = DataLoader(valX_dataset, batch_size=batch_size)\n",
        "testX_loader = DataLoader(testX_dataset, batch_size=batch_size)\n",
        "\n",
        "trainX1_loader = DataLoader(trainX1_dataset, batch_size=batch_size, shuffle=True)\n",
        "valX1_loader = DataLoader(valX1_dataset, batch_size=batch_size)\n",
        "testX1_loader = DataLoader(testX1_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [],
      "source": [
        "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = SoftMaxRegression().to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6716, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for x_batch, y_batch in trainX_loader:\n",
        "    \n",
        "    x_batch = x_batch.to(device).float()\n",
        "    y_batch = y_batch.to(device).squeeze().long()\n",
        "\n",
        "    y_hat = model(x_batch)\n",
        "    \n",
        "    ll = loss_fn(y_hat, y_batch)\n",
        "    print(ll)\n",
        "\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrapper for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Builds function that performs a step in the train loop\n",
        "    def train_step(x, y):\n",
        "        # Sets model to TRAIN mode\n",
        "        model.train()\n",
        "        # Makes predictions\n",
        "        yhat = model(x)\n",
        "        # Computes loss\n",
        "        loss = loss_fn(yhat, y)\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "    \n",
        "    # Returns the function that will be called inside the train loop\n",
        "    return train_step\n",
        "\n",
        "# Creates the train_step function for our model, loss function and optimizer\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SoftMaxRegression(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear): Linear(in_features=80, out_features=40, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    if type(m) ==  nn.Linear: \n",
        "        nn.init.normal_(m.weight,std=0.1)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### X Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('linear.weight', tensor([[ 0.2766, -0.7482,  0.3460,  ...,  1.3896,  0.6222, -0.3437],\n",
            "        [-0.9334, -0.9530, -0.1517,  ...,  2.2873,  0.6994, -0.9530],\n",
            "        [-1.2695,  0.1686,  0.4878,  ...,  3.5714,  1.1062,  3.3771],\n",
            "        ...,\n",
            "        [-1.6300, -1.1233,  0.8472,  ..., -0.8287,  0.1888,  1.3088],\n",
            "        [-1.1805, -0.8170,  0.6512,  ..., -3.2745, -2.8846,  1.8592],\n",
            "        [-1.2067, -0.0055, -0.7146,  ..., -0.5690,  0.2635,  1.4074]])), ('linear.bias', tensor([-1.9136, -1.7434,  0.0486, -0.7928,  0.2473,  0.0783, -1.0463, -1.9332,\n",
            "         0.3119, -0.4900,  0.5515, -1.6987, -2.4449, -0.7013, -1.8152, -2.0311,\n",
            "         1.7987, -1.8890, -1.7379, -0.4570, -0.8142, -2.1240, -0.8988, -0.4844,\n",
            "         1.5898, -1.3106,  0.0144,  0.0520,  1.5515, -2.0850,  0.9667, -0.4933,\n",
            "        -2.0139, -1.3049,  0.5284, -2.2870, -0.2663, -0.6619, -1.0919, -0.8248]))])\n"
          ]
        }
      ],
      "source": [
        "model.apply(init_weights)   #always good to initialize in the beginning\n",
        "n_epochs = 10\n",
        "losses = []\n",
        "test_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in trainX_loader:\n",
        "        x_batch = x_batch.to(device).float()\n",
        "        y_batch = y_batch.to(device).squeeze().long()\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # torch no_grad makes sure that the nested-below computations happen without gradients, \n",
        "    # since these are not needed for evaluation\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in testX_loader:\n",
        "            x_test = x_test.to(device).float()\n",
        "            y_test = y_test.to(device).squeeze().long()\n",
        "            \n",
        "            model.eval()\n",
        "    \n",
        "            yhat = model(x_test)\n",
        "            test_loss = loss_fn(yhat, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "print(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### X1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('linear.weight', tensor([[ 0.4067, -0.8577,  0.2374,  ..., -0.8811,  0.7289, -0.3836],\n",
            "        [ 0.0072,  1.9390,  0.9827,  ...,  0.2360,  0.1371, -0.3913],\n",
            "        [ 0.0086, -0.4924, -0.6045,  ...,  0.1279,  0.3803,  1.0905],\n",
            "        ...,\n",
            "        [ 1.2061,  0.3653, -0.6058,  ...,  0.5466, -0.9043, -0.1825],\n",
            "        [-0.9307,  0.0058, -1.0272,  ..., -0.3458, -1.5795, -0.2755],\n",
            "        [ 0.9841,  0.4441, -0.5828,  ..., -0.6786,  0.9929,  2.2160]])), ('linear.bias', tensor([-1.9613, -1.7140,  0.1235, -0.6326,  0.2062,  0.2206, -0.9908, -1.9010,\n",
            "         0.4361, -0.4371,  0.5993, -1.7063, -2.4117, -0.7616, -1.7749, -1.9527,\n",
            "         1.5822, -1.8806, -1.7650, -0.5999, -0.7322, -2.0306, -0.7876, -0.2844,\n",
            "         1.4646, -1.3631,  0.0472, -0.0031,  1.4119, -2.0040,  0.9942, -0.5587,\n",
            "        -1.9042, -1.2872,  0.6219, -2.2382, -0.2973, -0.5838, -0.8423, -0.7829]))])\n"
          ]
        }
      ],
      "source": [
        "model.apply(init_weights)   #always good to initialize in the beginning\n",
        "n_epochs = 10\n",
        "losses = []\n",
        "test_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in trainX1_loader:\n",
        "        x_batch = x_batch.to(device).float()\n",
        "        y_batch = y_batch.to(device).squeeze().long()\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # torch no_grad makes sure that the nested-below computations happen without gradients, \n",
        "    # since these are not needed for evaluation\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in testX1_loader:\n",
        "            x_test = x_test.to(device).float()\n",
        "            y_test = y_test.to(device).squeeze().long()\n",
        "            \n",
        "            model.eval()\n",
        "    \n",
        "            yhat = model(x_test)\n",
        "            test_loss = loss_fn(yhat, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "print(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(net, test_iter):  \n",
        "    \n",
        "    n_samples = 0 \n",
        "    n_correct = 0\n",
        "    model.eval()\n",
        "    for X, y in test_iter:\n",
        "        X = X.to(device).float()\n",
        "        y = y.to(device)\n",
        "        \n",
        "        trues = y\n",
        "        preds = model(X).argmax(axis=1)\n",
        "        \n",
        "        n_samples = n_samples + y.shape[0]\n",
        "        n_correct = n_correct + (trues==preds).sum()\n",
        "        break\n",
        "    \n",
        "    accuracy_tensor = n_correct/n_samples\n",
        "    return accuracy_tensor.item()*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### X Dataset Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainX Accuracy:  16.40625 %\n",
            "TestX Accuracy:  16.796875 %\n"
          ]
        }
      ],
      "source": [
        "print('TrainX Accuracy: ',accuracy(model,trainX_loader),\"%\")\n",
        "print('TestX Accuracy: ',accuracy(model,testX_loader),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### X1 Dataset Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainX1 Accuracy:  51.953125 %\n",
            "TestX1 Accuracy:  47.265625 %\n"
          ]
        }
      ],
      "source": [
        "print('TrainX1 Accuracy: ',accuracy(model,trainX1_loader),\"%\")\n",
        "print('TestX1 Accuracy: ',accuracy(model,testX1_loader),\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "jlICv7kW1qte"
      },
      "outputs": [],
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 8)\n",
        "\n",
        "# G[2] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDmA1ZZL152G"
      },
      "source": [
        "## <font color = 'blue'> Question 3. Maximize the accuracy on the two datasets </font>\n",
        "\n",
        "Augment your classifier from Question-2 with any number and type of layers you want, with the goal to maximize the **validation** accuracy you achieve on the two datasets. Feel free to use any stopping criterion you want for the training process. The networks for $X$ and $X_1$ do not have be of the same architecture.\n",
        "\n",
        "Show your code, and add a text cell summarizing your idea and findings. Finally apply your models to the **test** set, and report the accuracy. Feel free to discuss your validation accuracy on Canvas. Also please avoid looking at the test set, until the very end.\n",
        "\n",
        "**Rubric**: All complete answers get 8 points, and the **top 5** test accuracies reported get an extra 10\\% in the final quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class EnhancedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(80, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 40)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = EnhancedNet().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### X Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "ySrVeueW31b_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping early due to increasing validation loss.\n"
          ]
        }
      ],
      "source": [
        "# Training Loop with Early Stopping\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(100): \n",
        "    model.train()\n",
        "    for X_batch, y_batch in trainX_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = sum(loss_fn(model(Xb.to(device)), yb.to(device)) for Xb, yb in valX_loader) / len(valX_loader)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Stopping early due to increasing validation loss.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainX Accuracy:  41.40625 %\n",
            "ValX Accuracy:  49.21875 %\n",
            "TestX Accuracy:  47.65625 %\n"
          ]
        }
      ],
      "source": [
        "print('TrainX Accuracy: ',accuracy(model,trainX_loader),\"%\")\n",
        "print('ValX Accuracy: ',accuracy(model,valX_loader),\"%\")\n",
        "print('TestX Accuracy: ',accuracy(model,testX_loader),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### X1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping early due to increasing validation loss.\n"
          ]
        }
      ],
      "source": [
        "# Training Loop with Early Stopping\n",
        "best_val_loss = float('inf')\n",
        "early_stopping_patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(100): \n",
        "    model.train()\n",
        "    for X_batch, y_batch in trainX1_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = sum(loss_fn(model(Xb.to(device)), yb.to(device)) for Xb, yb in valX1_loader) / len(valX1_loader)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Stopping early due to increasing validation loss.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainX1 Accuracy:  7.8125 %\n",
            "ValX1 Accuracy:  10.15625 %\n",
            "TestX1 Accuracy:  9.765625 %\n"
          ]
        }
      ],
      "source": [
        "print('TrainX1 Accuracy: ',accuracy(model,trainX1_loader),\"%\")\n",
        "print('ValX1 Accuracy: ',accuracy(model,valX1_loader),\"%\")\n",
        "print('TestX1 Accuracy: ',accuracy(model,testX1_loader),\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summery of Ideas\n",
        "\n",
        "- Adding more layers and introducing non-linearity with ReLU activation functions helped in capturing complex patterns in the dataset. Dropout and Batch Normalization were key to prevent overfitting.\n",
        "- The learning rate scheduler allowed the model to make large updates initially and fine-tune with smaller updates as training progressed, improving convergence.\n",
        "- Implementing early stopping prevented overfitting to the training data by halting the training process when the validation loss started to increase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcYS1afK31cS"
      },
      "outputs": [],
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 8)\n",
        "\n",
        "# G[3] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZHVWBIjIuoy"
      },
      "outputs": [],
      "source": [
        "# total score\n",
        "max_score = 20\n",
        "$inal_score = sum(G)*(100/max_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
